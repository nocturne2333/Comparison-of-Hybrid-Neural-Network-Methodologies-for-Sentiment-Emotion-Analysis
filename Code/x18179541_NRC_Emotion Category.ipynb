{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "x18179541_Consolidated_Code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5ntCEY5BaAI",
        "colab_type": "text"
      },
      "source": [
        "#     **Data Pre-Processing, NLP Techniques, applications of LSTM & CNN Models**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXfwD0xtH1S-",
        "colab_type": "text"
      },
      "source": [
        "## **Data Cleaning**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGoip9eipaLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # Read file from gdrive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "df=pd.read_csv('drive/My Drive/August01_Tweets_Final.csv')\n",
        "df=df.drop_duplicates().reset_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGnkDHYJpgYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lzSdOowpgbb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove html\n",
        "from bs4 import BeautifulSoup\n",
        "def remove_html(text):\n",
        "  soup=BeautifulSoup(text, 'lxml')\n",
        "  html_free=soup.get_text()\n",
        "  return html_free"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f80GTbaPpgd0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove Punctuation\n",
        "import string\n",
        "def remove_punctuation(text):\n",
        "  no_punct = \"\".join([c for c in text if c not in string.punctuation])\n",
        "  return no_punct"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv5YmNfOpmKe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove \\n character\n",
        "df['tweet']= df['tweet'].apply(lambda x:remove_punctuation(x)).apply(lambda x: x.replace('\\n', ' ')).apply(lambda x:remove_html(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxwpALLZpmII",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate Tokenizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer(r'\\w+')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9uN8WaPpmFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['tweet']= df['tweet'].apply(lambda x:tokenizer.tokenize(x.lower()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMMHHGc0pmC_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEWfe81bpmAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove Stop words\n",
        "from nltk.corpus import stopwords\n",
        "def remove_stopwords(text):\n",
        "  words = [w for w  in text if w not in stopwords.words('english')]\n",
        "  return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqElXk_9As3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['tweet']=df['tweet'].apply(lambda x:remove_stopwords(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzCeaxINAs0C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lemmatizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asDyIY91AsxV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_lemmatizer(text):\n",
        "  lem_text = [lemmatizer.lemmatize(i) for i in text]\n",
        "  return lem_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-jgHKWfAstR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['tweet']=df['tweet'].apply(lambda x:word_lemmatizer(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKg-gVLvAsrG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# stemming\n",
        "from  nltk.stem.porter import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def word_stemmer(text):\n",
        "  stem_text = \" \".join([stemmer.stem(i) for i in text])\n",
        "  return stem_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqXkdxjeAsoq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['tweet']=df['tweet'].apply(lambda x:word_stemmer(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqY4pB7kA_y5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install emoji\n",
        "#!pip install nrclex"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IDRkglyAsmA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# removing '&gt;'\n",
        "df['tweet'] = df['tweet'].apply(lambda x: x.replace('&gt;', ''))\n",
        "# Remove Hyperlinks\n",
        "df['tweet']= df['tweet'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
        "# remove ' s ' that was created after removing punctuations\n",
        "df['tweet'] = df['tweet'].apply(lambda x: str(x).replace(\" s \", \" \"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvcQZrBPAsjG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Emoji removal\n",
        "import emoji\n",
        "#remove the emoji\n",
        "def deEmojify(inputString):\n",
        "    return inputString.encode('ascii', 'ignore').decode('ascii')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXj_XjMLBIrq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['tweet'] = df['tweet'].apply(lambda x: deEmojify(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AozPuPcBIpg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nrclex import NRCLex\n",
        "senti_scores_list = []\n",
        "max_key = []\n",
        "\n",
        "for words in df['tweet']:\n",
        "  senti_scores = NRCLex(words)\n",
        "  senti_scores_list.append(senti_scores.raw_emotion_scores)\n",
        "\n",
        "for a in senti_scores_list:\n",
        "  if a != {}:\n",
        "    max_key.append(max(a, key=a.get))\n",
        "  else:\n",
        "    max_key.append('no_sentiment')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNDZz7vUBInJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['sentiment'] = max_key"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7D1KBsxBIlZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Store cleaned_data in drive\n",
        "df.to_csv('cleaned_data.csv')\n",
        "!cp cleaned_data.csv \"drive/My Drive/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm3QQtD1BIjF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set(font_scale=1.4)\n",
        "df['sentiment'].value_counts().plot(kind='bar', figsize=(20, 6), rot=0)\n",
        "plt.xlabel(\"sentiment\", labelpad=14)\n",
        "plt.ylabel(\"Count of Sentiment\", labelpad=14)\n",
        "plt.title(\"Count of Sentiments by Category\", y=1.02);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFP-PDudBIhF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleaned_data = df[(df['sentiment']!='anticip')].reset_index()\n",
        "del cleaned_data['level_0']\n",
        "del cleaned_data['index']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "em0ZMFaFBIey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set(font_scale=1.4)\n",
        "cleaned_data['sentiment'].value_counts().plot(kind='bar', figsize=(20, 6), rot=0)\n",
        "plt.xlabel(\"sentiment\", labelpad=14)\n",
        "plt.ylabel(\"Count of Sentiment\", labelpad=14)\n",
        "plt.title(\"Count of Sentiments by Category\", y=1.02);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QA_7t4sPBIcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Store cleaned_data in drive\n",
        "cleaned_data.to_csv('final_cleaned_data.csv', index=False)\n",
        "!cp final_cleaned_data.csv \"drive/My Drive/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AML616j3BtAV",
        "colab_type": "text"
      },
      "source": [
        "## **Building LSTM Model**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tm6Y7Ol5BUYY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read file from gdrive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "data=pd.read_csv('drive/My Drive/final_cleaned_data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRcQ-Xy-BUUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The maximum number of words to be used. (most frequent)\n",
        "MAX_NB_WORDS = 50000\n",
        "# Max number of words in each complaint.\n",
        "MAX_SEQUENCE_LENGTH = 250\n",
        "# This is fixed.\n",
        "EMBEDDING_DIM = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9cSiwqSCMVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
        "tokenizer.fit_on_texts(data['tweet'].values)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuBhJ4LuCMRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "X = tokenizer.texts_to_sequences(data['tweet'].values)\n",
        "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensor:', X.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABmwFO9DCMPG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Y = pd.get_dummies(data['sentiment']).values\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "mlb = LabelEncoder()\n",
        "sentiment = data['sentiment'].to_numpy()\n",
        "mlb.fit(sentiment)\n",
        "Y = mlb.transform(sentiment)\n",
        "print('Shape of label tensor:', Y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVhHpJKMCMM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DataSplit\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, stratify=Y, random_state = 42)\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1F5fcaSCMKr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding,SpatialDropout1D\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "model = Sequential()\n",
        "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(LSTM(100, dropout=0.5, recurrent_dropout=0.5))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "epochs = 5\n",
        "batch_size = 64\n",
        "\n",
        "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FAGuqUdCUoD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accr = model.evaluate(X_test,Y_test)\n",
        "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiT3hcTBCUlt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.title('Loss')\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d42EZJIvCUjN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.title('Accuracy')\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN_RQLGcGZ3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predictions for test data\n",
        "predictions = model.predict_classes(X_test, \n",
        "                            batch_size=100, \n",
        "                            verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Gdhkvr3Gbu8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#confusion matrix for the test data\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=True,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    import itertools\n",
        "    if normalize:\n",
        "        cm = (cm.astype('float') / cm.sum(axis=1)[:, np.newaxis])*100\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "\n",
        "# confusion matrix creation\n",
        "#from sklearn.metrics import confusion_matrix\n",
        "#labels = mlb.classes_\n",
        "#cnf_matrix = confusion_matrix(y_true, y_pred,labels=labels)\n",
        "#np.set_printoptions(precision=2)\n",
        "y_true = mlb.inverse_transform(Y_test)\n",
        "y_pred = mlb.inverse_transform(predictions)\n",
        "\n",
        "import scikitplot as skplt\n",
        "# Plot confusion matrix\n",
        "#plt.figure()\n",
        "skplt.metrics.plot_confusion_matrix(y_true, y_pred,\n",
        "                     title='Confusion matrix', figsize = (20,15), text_fontsize='medium', cmap='Reds')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4CxshjAGeT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#! pip install scikit-plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQJj1cafGuCM",
        "colab_type": "text"
      },
      "source": [
        "## **Building CNN Model**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qm6AEx2yGpv4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read file from gdrive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "data = pd.read_csv('drive/My Drive/final_cleaned_data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRc0KbgwG2ib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train-Test Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "training_bs, test = train_test_split(data, \n",
        "                                         test_size=0.10, \n",
        "                                         random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kko8Ig6LG2f1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_bs.loc[training_bs['sentiment']=='positive', 'sentiment_score'] = int(1)\n",
        "training_bs.loc[training_bs['sentiment']=='negative', 'sentiment_score'] = int(2)\n",
        "training_bs.loc[training_bs['sentiment']=='no_sentiment', 'sentiment_score'] = int(3)\n",
        "training_bs.loc[training_bs['sentiment']=='sadness', 'sentiment_score'] = int(4)\n",
        "training_bs.loc[training_bs['sentiment']=='fear', 'sentiment_score'] = int(5)\n",
        "training_bs.loc[training_bs['sentiment']=='trust', 'sentiment_score'] = int(6)\n",
        "training_bs.loc[training_bs['sentiment']=='anger', 'sentiment_score'] = int(7)\n",
        "training_bs.loc[training_bs['sentiment']=='surprise', 'sentiment_score'] = int(8)\n",
        "training_bs.loc[training_bs['sentiment']=='joy', 'sentiment_score'] = int(9)\n",
        "training_bs.loc[training_bs['sentiment']=='disgust', 'sentiment_score'] = int(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-XqCn1LG2eF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## build training vocabulary and get maximum training sentence length and total number of words training data\n",
        "all_training_words = ''.join([ word for tokens in training_bs[\"tweet\"] for word in tokens])\n",
        "training_sentence_lengths = [len(tokens) for tokens in training_bs[\"tweet\"]]\n",
        "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
        "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obMG0u_sG2bA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Build testing vocabulary and get maximum testing sentence length and total number of words in testing data\n",
        "\n",
        "all_test_words = ''.join([word for tokens in test[\"tweet\"] for word in tokens])\n",
        "test_sentence_lengths = [len(tokens) for tokens in test[\"tweet\"]]\n",
        "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
        "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
        "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXvAYHTnG2Yo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Loading Google News Word2Vec model\n",
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "from gensim.models import KeyedVectors\n",
        "word2vec_path = 'GoogleNews-vectors-negative300.bin.gz'\n",
        "word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngBUwEHlG2Wb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUYYpIqeG2T1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenization\n",
        "tokenizer.fit_on_texts(training_bs[\"tweet\"].tolist())\n",
        "training_sequences = tokenizer.texts_to_sequences(training_bs[\"tweet\"].tolist())\n",
        "train_word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(train_word_index))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSuL-3fRG_RR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 500\n",
        "EMBEDDING_DIM = 300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5VG8nM5G_O_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "train_cnn_data = pad_sequences(training_sequences, \n",
        "                               maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoWF7aAyG_Mv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_embedding_weights = np.zeros((len(train_word_index)+1, \n",
        " EMBEDDING_DIM))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ucdq-4SEHI5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Word2Vec word Embedding\n",
        "for word,index in train_word_index.items():\n",
        " train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
        "print(train_embedding_weights.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpjVvPD0HI2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build Model\n",
        "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
        " \n",
        "    embedding_layer = Embedding(num_words,\n",
        "                            embedding_dim,\n",
        "                            weights=[embeddings],\n",
        "                            input_length=max_sequence_length,\n",
        "                            trainable=False)\n",
        "    \n",
        "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
        "    embedded_sequences = embedding_layer(sequence_input)\n",
        "    convs = []\n",
        "    filter_sizes = [2,3,4,5,6]\n",
        "    for filter_size in filter_sizes:\n",
        "        l_conv = Conv1D(filters=300, \n",
        "                        kernel_size=filter_size, \n",
        "                        activation='relu')(embedded_sequences)\n",
        "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
        "        convs.append(l_pool)\n",
        "    l_merge = concatenate(convs, axis=1)\n",
        "    x = Dropout(0.1)(l_merge)  \n",
        "    x = Dense(128, activation='softmax')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    preds = Dense(labels_index, activation='softmax')(x)\n",
        "    model = Model(sequence_input, preds)\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['acc'])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJVaxuDuHSBh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_names = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9x1kr0lHR_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = train_cnn_data\n",
        "y_tr = training_bs['sentiment_score'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afoUUgL6HR7Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model Function\n",
        "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
        "from keras.models import Model\n",
        "model = ConvNet(train_embedding_weights, \n",
        "                MAX_SEQUENCE_LENGTH, \n",
        "                len(train_word_index)+1, \n",
        "                EMBEDDING_DIM, \n",
        "                len(list(label_names)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MY30lVOdHR4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 5\n",
        "batch_size = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AClcud_bHaNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hist = model.fit(x_train, y_tr, epochs=num_epochs, validation_split=0.1, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de-8Ne9zHaJ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test.loc[test['sentiment']=='positive', 'sentiment_score'] = int(1)\n",
        "test.loc[test['sentiment']=='negative', 'sentiment_score'] = int(2)\n",
        "test.loc[test['sentiment']=='no_sentiment', 'sentiment_score'] = int(3)\n",
        "test.loc[test['sentiment']=='sadness', 'sentiment_score'] = int(4)\n",
        "test.loc[test['sentiment']=='fear', 'sentiment_score'] = int(5)\n",
        "test.loc[test['sentiment']=='trust', 'sentiment_score'] = int(6)\n",
        "test.loc[test['sentiment']=='anger', 'sentiment_score'] = int(7)\n",
        "test.loc[test['sentiment']=='surprise', 'sentiment_score'] = int(8)\n",
        "test.loc[test['sentiment']=='joy', 'sentiment_score'] = int(9)\n",
        "test.loc[test['sentiment']=='disgust', 'sentiment_score'] = int(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WST_fIizHaHd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_sequences = tokenizer.texts_to_sequences(test[\"tweet\"].tolist())\n",
        "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bW82gfvEHfUF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(test_cnn_data, \n",
        "                            batch_size=1024, \n",
        "                            verbose=1)\n",
        "labels = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]\n",
        "prediction_labels=[]\n",
        "for p in predictions:\n",
        "    prediction_labels.append(labels[np.argmax(p)])\n",
        "sum(test.sentiment_score==prediction_labels)/len(prediction_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzEvmIGTHfQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#confusion matrix for the test data\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "mlb = LabelEncoder()\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=True,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    import itertools\n",
        "    if normalize:\n",
        "        cm = (cm.astype('float') / cm.sum(axis=1)[:, np.newaxis])*100\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "\n",
        "# confusion matrix creation\n",
        "#y_true = mlb.inverse_transform(y_tr)\n",
        "#y_pred = mlb.inverse_transform(predictions)\n",
        "\n",
        "import scikitplot as skplt\n",
        "# Plot confusion matrix\n",
        "#plt.figure()\n",
        "skplt.metrics.plot_confusion_matrix(test['sentiment_score'], prediction_labels,\n",
        "                     title='Confusion matrix', figsize = (20,15), text_fontsize='medium', cmap='Reds')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXJrOYz9HfOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}